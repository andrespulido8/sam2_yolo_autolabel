{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3b1c46-9f5c-41c1-9101-85db8709ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7a0db5-7f04-4845-8b11-684fe6e9f7f2",
   "metadata": {},
   "source": [
    "# Auto labeling based on video segmentation with SAM 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ba7875-35e5-478b-b8ba-4b48e121dec7",
   "metadata": {},
   "source": [
    "This notebook shows how to use SAM 2 for interactive segmentation in videos and then extract the bounding boxes for YOLO training with the output. It will cover the following:\n",
    "\n",
    "- adding clicks (or box) on a frame to get and refine _masklets_ (spatio-temporal masks)\n",
    "- propagating clicks (or box) to get _masklets_ throughout the video\n",
    "- segmenting and tracking multiple objects at the same time\n",
    "- extracting bounding boxes for YOLO training\n",
    "\n",
    "We use the terms _segment_ or _mask_ to refer to the model prediction for an object on a single frame, and _masklet_ to refer to the spatio-temporal masks across the entire video. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887b90f-6576-4ef8-964e-76d3a156ccb6",
   "metadata": {},
   "source": [
    "This notebook is based on the example notebook from the SAM2 repository, which you can run to get more familiarized the the SAM2 features. To run it, use the following google colab badge:  \n",
    "  \n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/facebookresearch/sam2/blob/main/notebooks/video_predictor_example.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26616201-06df-435b-98fd-ad17c373bb4a",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491a127-4c01-48f5-9dc5-f148a9417fdf",
   "metadata": {},
   "source": [
    "If running locally using jupyter, first install `sam2` in your environment using the [installation instructions](https://github.com/facebookresearch/sam2#installation) in the repository.\n",
    "\n",
    "If running from Google Colab, set `using_colab=True` below and run the cell. In Colab, be sure to select 'GPU' under 'Edit'->'Notebook Settings'->'Hardware accelerator'. Note that it's recommended to use **A100 or L4 GPUs when running in Colab** (T4 GPUs might also work, but could be slow and might run out of memory in some cases).\n",
    "\n",
    "**NOTE**: the google colab functionality has not yet been implemented for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f74c53be-aab1-46b9-8c0b-068b52ef5948",
   "metadata": {},
   "outputs": [],
   "source": [
    "using_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d824a4b2-71f3-4da3-bfc7-3249625e6730",
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_colab:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(\"PyTorch version:\", torch.__version__)\n",
    "    print(\"Torchvision version:\", torchvision.__version__)\n",
    "    print(\"CUDA is available:\", torch.cuda.is_available())\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install opencv-python matplotlib\n",
    "    !{sys.executable} -m pip install 'git+https://github.com/facebookresearch/sam2.git'\n",
    "\n",
    "    !mkdir -p videos\n",
    "    !wget -P videos https://dl.fbaipublicfiles.com/segment_anything_2/assets/bedroom.zip\n",
    "    !unzip -d videos videos/bedroom.zip\n",
    "\n",
    "    !mkdir -p ../checkpoints/\n",
    "    !wget -P ../checkpoints/ https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6aa9d-487f-4207-b657-8cff0902343e",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5318a85-5bf7-4880-b2b3-15e4db24d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "# get path where this file is located\n",
    "# notebooks.sam2_yolo_autolabel.\n",
    "from notebooks.sam2_yolo_autolabel.yolo_utils import get_bounding_box, save_bboxes_to_yolo_format, read_yolo_labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ba49d8-8c22-4eba-a2ab-46eee839287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8e0779-751f-4224-9b04-ed0f0b406500",
   "metadata": {},
   "source": [
    "### Loading the SAM 2 video predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5f3245e-b4d6-418b-a42a-a67e0b3b5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a5320fe-06d7-45b8-b888-ae00799d07fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, obj_id=None, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        cmap = plt.get_cmap(\"tab10\")\n",
    "        cmap_idx = 0 if obj_id is None else obj_id\n",
    "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=200):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "\n",
    "def show_box(box, ax, edgecolor='green'):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor=edgecolor, facecolor=(0, 0, 0, 0), lw=2))\n",
    "\n",
    "\n",
    "def get_leading_zeros(s):\n",
    "    leading_zeros = ''\n",
    "    for char in s:\n",
    "        if char == '0':\n",
    "            leading_zeros += char\n",
    "        else:\n",
    "            break\n",
    "    return leading_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa751-b7cd-451e-9ded-fb98bf4bdfad",
   "metadata": {},
   "source": [
    "#### Extract frames of video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c6af6-e18d-4939-beaf-2bc00f94a724",
   "metadata": {},
   "source": [
    "The video should be actually stored as a list of JPEG frames with filenames like `<frame_index>.jpg`.\n",
    "\n",
    "To extract extract the frames for other videos, use ffmpeg (https://ffmpeg.org/) as follows:\n",
    "```\n",
    "ffmpeg -i <your_video>.mp4 -r 1 -q:v 2 -start_number 0 <output_dir>/'%05d.jpg'\n",
    "```\n",
    "where `-r 1` says to get 1 frame every second, `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915c148b",
   "metadata": {},
   "source": [
    "Make sure you use jpg images, as while ffmpeg will process png's or other formats, the model only works with jpgs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2756d9",
   "metadata": {},
   "source": [
    "### Select the frames and labels directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c87ca-fd1a-4011-9609-e8be1cbe3230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n",
    "#video_dir = \"/home/basestation/Documents/sam2/notebooks/videos/20241003_autonomy_park/20241003_autonomy_park_drp/batch_0\"\n",
    "video_dir = \"/home/basestation/Documents/sam2/notebooks/videos/20241021_RAITE/20241021_RAITE_pumba/batch_0\"\n",
    "labels_dir = \"/home/basestation/Documents/sam2/notebooks/videos/20241021_RAITE/20241021_RAITE_pumba/labels\"\n",
    "batch_num = int(video_dir[-1])\n",
    "is_first_run = True if batch_num == 0 else False \n",
    "print(\"is_first_run: \", is_first_run)\n",
    "if is_first_run:\n",
    "    print(\"Running for first time, select the points in following cells\")\n",
    "else:\n",
    "    print(\"Automatically getting bounding box of last frame\")\n",
    "\n",
    "# labels\n",
    "if not os.path.exists(labels_dir):\n",
    "    os.mkdir(labels_dir)\n",
    "\n",
    "# scan all the JPEG frame names in this directory\n",
    "frame_names = [\n",
    "    p for p in os.listdir(video_dir)\n",
    "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n",
    "]\n",
    "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "print(f\"found {len(frame_names)} frames\")\n",
    "leading_zeros = get_leading_zeros(frame_names[0].split('.')[0])\n",
    "\n",
    "if not is_first_run:\n",
    "    # find the last frame in the last batch and include it in this batch\n",
    "    last_frame_last_batch = int(frame_names[0].split('.')[0]) - 1\n",
    "    print(\"starting frame: \", last_frame_last_batch)\n",
    "    # move the image last_frame_last_batch.jpg from last batch to this batch\n",
    "    os.rename(f\"{video_dir}/../batch_\" + str(batch_num - 1) + f\"/{leading_zeros}{last_frame_last_batch}.jpg\",\n",
    "               f\"{video_dir}/{leading_zeros}{last_frame_last_batch}.jpg\")\n",
    "    frame_names.insert(0, f\"{leading_zeros}{last_frame_last_batch}.jpg\")\n",
    "else:\n",
    "    last_frame_last_batch = 0\n",
    "\n",
    "# take a look the first video frame in this batch\n",
    "frame_batch_idx = 0\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_batch_idx}\") if is_first_run else plt.title(f\"frame {frame_batch_idx + last_frame_last_batch}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_batch_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b01e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_width, image_height = Image.open(os.path.join(video_dir, frame_names[0])).size\n",
    "print(f\"Image width: {image_width}, Image height: {image_height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff46b10-c17a-4a26-8004-8c6d80806b0a",
   "metadata": {},
   "source": [
    "#### Initialize the inference state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594ac71-a6b9-461d-af27-500fa1d1a420",
   "metadata": {},
   "source": [
    "SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n",
    "\n",
    "During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967aed3-eb82-4866-b8df-0f4743255c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_state = predictor.init_state(video_path=video_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d3127-67b2-45d2-9f32-8fe3e10dc5eb",
   "metadata": {},
   "source": [
    "Note: if you have run any previous tracking using this `inference_state`, please reset it first via `reset_state`.\n",
    "\n",
    "(The cell below is just for illustration; it's not needed to call `reset_state` here as this `inference_state` is just freshly initialized above.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2646a1d-3401-438c-a653-55e0e56b7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.reset_state(inference_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95158714-86d7-48a9-8365-b213f97cc9ca",
   "metadata": {},
   "source": [
    "SAM 2 can segment and track two or more objects at the same time. One way, of course, is to do them one by one. However, it would be more efficient to batch them together (e.g. so that we can share the image features between objects to reduce computation costs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9ac57-b14a-4237-828d-927e422c518b",
   "metadata": {},
   "source": [
    "Add the first object with a **positive click** on frame 0. There are also negative clicks, which are used to exclude regions from the mask.\n",
    "\n",
    "We assign it to object id `0` (it can be arbitrary integers, and only needs to be unique for each object to track), which is passed to the `add_new_points_or_box` API to distinguish the object we are clicking upon.\n",
    "\n",
    "To include the following objects, keep including positive clicks with another object id. \n",
    "There is also the option to include positive clicks in another frame. See the example notebook in SAM2 on how to do this. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca1bde-62a4-40e6-98e4-15606441e52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo labels are 0: background, 1: drones, 2: ground vehicles \n",
    "# change the following dictionary to m40ap the sam2 object id (key) to the yolo label id (value).  \n",
    "obj_id_to_yolo_label = {0: 2,  \n",
    "                        }\n",
    "\n",
    "prompts = {}  # hold all the clicks we add for visualization\n",
    "\n",
    "ann_frame_idx = 0  # the frame index we interact with\n",
    "\n",
    "if is_first_run:\n",
    "\n",
    "    ann_obj_id = 0  # give a unique id to each object we interact with (it can be any integers)\n",
    "    points = np.array([[210,700]], dtype=np.float32)\n",
    "    labels = np.array([1], np.int32) \n",
    "    #points = np.array([[1530,580],[1500,620],[1860, 710],[890,620],[900,470]], dtype=np.float32)\n",
    "    #labels = np.array([1,1,1,1,1], np.int32)  # the label for each point, 1 for positive, 0 for negative\n",
    "    prompts[ann_obj_id] = points, labels\n",
    "    _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "        inference_state=inference_state,\n",
    "        frame_idx=ann_frame_idx,\n",
    "        obj_id=ann_obj_id,\n",
    "        points=points,\n",
    "        labels=labels,\n",
    "    )\n",
    "\n",
    "    # if there are more than one object in frame, add more points or boxes\n",
    "\n",
    "    # you can add more than one point (positive or negative) per object\n",
    "    #points = np.array([[340, 170], [340, 140]], dtype=np.float32)\n",
    "    #labels = np.array([1,1], np.int32)\n",
    "    #prompts[ann_obj_id] = points, labels\n",
    "    #_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "    #    inference_state=inference_state,\n",
    "    #    frame_idx=ann_frame_idx,\n",
    "    #    obj_id=ann_obj_id,\n",
    "    #    points=points,\n",
    "    #    labels=labels,\n",
    "    #)\n",
    "else: \n",
    "    yolo_labels = read_yolo_labels(labels_dir + \"/\" + leading_zeros + str(last_frame_last_batch + ann_frame_idx) + '.txt', image_width, image_height)\n",
    "    print(\"yolo_labels used to find object: \", yolo_labels)\n",
    "\n",
    "    obj_index = 0\n",
    "    for _, bbox in yolo_labels:\n",
    "        box = np.array(bbox, dtype=np.float32)\n",
    "        print(box)\n",
    "        _, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=ann_frame_idx,\n",
    "            obj_id=obj_index,\n",
    "            box=box,\n",
    "        )\n",
    "        obj_index += 1\n",
    "\n",
    "# show the results on the current (interacted) frame on all objects\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {ann_frame_idx}\") if is_first_run else plt.title(f\"frame {ann_frame_idx + last_frame_last_batch}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n",
    "for i, out_obj_id in enumerate(out_obj_ids):\n",
    "    if is_first_run:\n",
    "        show_points(*prompts[out_obj_id], plt.gca())\n",
    "    else:\n",
    "        print(yolo_labels[i][1])\n",
    "        show_box(yolo_labels[i][1], plt.gca())\n",
    "    show_mask((out_mask_logits[i] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92dadd2",
   "metadata": {},
   "source": [
    "### Propagate Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bd73de-d669-41c8-b6ba-943883f0caa2",
   "metadata": {},
   "source": [
    "Now, we propagate the prompts for all objects to get their masklets throughout the video.\n",
    "\n",
    "Note: when there are multiple objects, the `propagate_in_video` API will return a list of masks for each object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17737191-d62b-4611-b2c6-6d0418a9ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run propagation throughout the video and collect the results in a dict\n",
    "video_segments = {}  # video_segments contains the per-frame segmentation results\n",
    "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n",
    "    video_segments[out_frame_idx] = {\n",
    "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n",
    "        for i, out_obj_id in enumerate(out_obj_ids)\n",
    "    }\n",
    "\n",
    "# render the segmentation results every few frames\n",
    "show_num_frames = 10\n",
    "vis_frame_stride = len(frame_names) // show_num_frames\n",
    "plt.close(\"all\")\n",
    "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.title(f\"frame {ann_frame_idx}\") if is_first_run else plt.title(f\"frame {out_frame_idx + last_frame_last_batch}\")\n",
    "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n",
    "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n",
    "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e142f72",
   "metadata": {},
   "source": [
    "### Extract yolo bounding boxes from segmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b0764-4145-41a6-87a5-00232121a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert length of video_segments is equal to the number of frames from frame_names\n",
    "assert len(video_segments) == len(frame_names)\n",
    "for frame_index, objects in video_segments.items():\n",
    "    frame_name = frame_names[frame_index].split(\".\")[0]\n",
    "    output_file = os.path.join(labels_dir, f\"{frame_name}.txt\")\n",
    "    bboxes = []\n",
    "    label_ids = [] \n",
    "    for obj_index, segmentation in objects.items():\n",
    "        if np.sum(np.any(segmentation, axis=0)) + np.sum(np.any(segmentation, axis=1)) > 0:\n",
    "            bboxes.append(get_bounding_box(segmentation[0]))\n",
    "            label_ids.append(obj_id_to_yolo_label[obj_index])\n",
    "    save_bboxes_to_yolo_format(image_width, image_height, bboxes, label_ids, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37202dd",
   "metadata": {},
   "source": [
    "#### Confirm the bounding box output is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e671d572",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_batch_idx = 10  # pick any arbitrary frame in this batch to visualize the results\n",
    "\n",
    "color = {0: 'red', 1: 'blue', 3: 'green'}  # for legend\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.title(f\"frame {frame_batch_idx}\") if is_first_run else plt.title(f\"frame {frame_batch_idx + last_frame_last_batch}\")\n",
    "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_batch_idx])))\n",
    "# determine how many zeroes to remove\n",
    "if is_first_run:\n",
    "    leng_idx = len(str(frame_batch_idx))\n",
    "else:\n",
    "    leng_idx= len(str(frame_batch_idx)) - len(str(last_frame_last_batch)) \n",
    "# remove the zeroes\n",
    "if leng_idx >= 1:\n",
    "    idx_leading_zeros = leading_zeros[:-leng_idx]\n",
    "else: \n",
    "    idx_leading_zeros = leading_zeros\n",
    "# read labels\n",
    "if is_first_run == False:\n",
    "    yolo_labels = read_yolo_labels(labels_dir + \"/\" + idx_leading_zeros + str(frame_batch_idx) + '.txt', image_width, image_height)\n",
    "else: \n",
    "    yolo_labels = read_yolo_labels(labels_dir + \"/\" + idx_leading_zeros + str(frame_batch_idx + last_frame_last_batch) + '.txt', image_width, image_height)\n",
    "\n",
    "print(\"labels: \", yolo_labels)\n",
    "count = 0\n",
    "for obj_index, segmentation in video_segments[frame_batch_idx].items():\n",
    "    box = np.array(yolo_labels[count][1], dtype=np.float32)\n",
    "    show_box(box, plt.gca(), edgecolor=color[obj_index])\n",
    "    count += 1\n",
    "# legend\n",
    "for obj_index, c in color.items():\n",
    "    plt.plot([], [], color=c, label=f\"Object {obj_index}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1073ff1",
   "metadata": {},
   "source": [
    "## Second Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145d0fd2",
   "metadata": {},
   "source": [
    "For the second batch and beyond, we can repeat the process above to add more objects and propagate them throughout the video but now\n",
    "instead of manually selecting the objects, we will use the bounding boxes from the last frame of the previous batch, include it in \n",
    "this batch and propagate it throughout the video. \n",
    "\n",
    "The only step to do this is change the directory name to specify the batch will be greater than 0. Then, we will use the bounding boxes \n",
    "automatically. \n",
    "\n",
    "**NOTE**: you should **reset the notebook before** loading the second batch because if not the GPU will run out of memory. This is the reason we split the video frames in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcd4a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
